{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ipq518geRcD"
      },
      "source": [
        "# Chapter 11 - Building a Causal Effect Inference Workflow with DoWhy\n",
        "\n",
        "The notebook is a code companion to chapter 11 of the book [Causal AI](https://www.manning.com/books/causal-ai) by [Robert Osazuwa Ness](https://www.linkedin.com/in/osazuwa/).  View the [book resources](https://www.altdeep.ai/causalaibook) to see other tutorials and book-related links.\n",
        "\n",
        "<a href=\"https://github.com/altdeep/causalML/blob/master/book/chapter%2011/Chapter_11_DoWhy_Causal_Effect_Workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvkoqysCeZpS"
      },
      "outputs": [],
      "source": [
        "!apt-get install graphviz libgraphviz-dev pkg-config    #A\n",
        "!pip install pygraphviz==1.12    #A\n",
        "!pip install dowhy==0.11\n",
        "!pip install econml==0.15\n",
        "\n",
        "#A DoWHy uses pygraphviz to visualize the DAG. This uses apt-get to first install the underlying graphviz package in Debian-based Linux environment like Google Colab. Try using  Chocolatey to install graphviz for Windows and Brew for MacOS."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 11.1: Build the Causal DAG\n",
        "\n",
        "We return to the online gaming DAG. We expand it with a few new variables.\n",
        "\n",
        "* Side-quest Group Assignment: 1 if a player was exposed to the mechanics that encouraged more side-quest engagement in the randomized experiment, 0 otherwise.\n",
        "* Customization Level: A score quantifying the player’s customizations of their character and the game environment.\n",
        "* Time Spent Playing (T): How much time the player has spent playing.\n",
        "* Prior Experience (Y): How much experience the player had prior to when they started playing the game.\n",
        "* Player Skill Level (S): A score of how well the player performs in game tasks.\n",
        "* Total inventory (V): The amount of game items the player has accumulated.\n",
        "\n",
        "![causal DAG with latent](https://github.com/altdeep/causalML/blob/master/book/chapter%2011/images/gamingDAG.png?raw=true)\n",
        "\n",
        "We are interested in finding the average treatment effect between Side-quest Engagement and In-game Purchases.\n",
        "\n",
        "First, let’s build the DAG and visualize the graph with the PyGraphviz library."
      ],
      "metadata": {
        "id": "DCBawB0QUz-r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ADuBXs015qN"
      },
      "outputs": [],
      "source": [
        "import pygraphviz as pgv    #A\n",
        "from IPython.display import Image   #B\n",
        "\n",
        "causal_graph = \"\"\"\n",
        "digraph {\n",
        "    \"Prior Experience\" -> \"Player Skill Level\";\n",
        "    \"Prior Experience\" -> \"Time Spent Playing\";\n",
        "    \"Time Spent Playing\" -> \"Player Skill Level\";\n",
        "    \"Guild Membership\" -> \"Side-quest Engagement\";\n",
        "    \"Guild Membership\" -> \"In-game Purchases\";\n",
        "    \"Player Skill Level\" -> \"Side-quest Engagement\";\n",
        "    \"Player Skill Level\" -> \"In-game Purchases\";\n",
        "    \"Time Spent Playing\" -> \"Side-quest Engagement\";\n",
        "    \"Time Spent Playing\" -> \"In-game Purchases\";\n",
        "    \"Side-quest Group Assignment\" -> \"Side-quest Engagement\";\n",
        "    \"Customization Level\" -> \"Side-quest Engagement\";\n",
        "    \"Side-quest Engagement\" -> \"Won Items\";\n",
        "    \"Won Items\" -> \"In-game Purchases\";\n",
        "    \"Won Items\" -> \"Total Inventory\";\n",
        "    \"In-game Purchases\" -> \"Total Inventory\";\n",
        "}\n",
        "\"\"\"     #C\n",
        "G = pgv.AGraph(string=causal_graph)    #C\n",
        "G.draw('/tmp/causal_graph.png', prog='dot')    #D\n",
        "Image('/tmp/causal_graph.png')    #E\n",
        "\n",
        "#A Download pygraphviz and related libraries.\n",
        "#B Optional import for visualizing the DAG in a Jupyter notebook\n",
        "#C Specify the DAG as a DOT language string and load a pygraphviz AGraph object from the string.\n",
        "#D Render the graph to a PNG file.\n",
        "#E Display the graph."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 11.2: Download and display the data\n",
        "\n",
        "Let's download our data and see what variables are in our observational distribution."
      ],
      "metadata": {
        "id": "KgEsn612b573"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHvs5CLMIOsf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/altdeep/causalML/master/datasets/online_game_example_do_why.csv\"    #A\n",
        ")\n",
        "print(data.columns)    #B\n",
        "\n",
        "#A Download an online gaming dataset.\n",
        "#B Print the variables"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 11.3: Instantiate an instance of DoWhy's CausalModel\n",
        "\n",
        "In our data, Prior Experience (Y) is unobserved.\n",
        "\n",
        "![causal DAG with latent](images/gamingDAGLatent.png)\n",
        "\n",
        "We use the CausalModel class to represent the DAG and tell us if the ATE is identified."
      ],
      "metadata": {
        "id": "XyTgNvyIcQEe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxGQ9hbYzUfR"
      },
      "outputs": [],
      "source": [
        "from dowhy import CausalModel    #A\n",
        "\n",
        "model = CausalModel(\n",
        "    data=data,    #B\n",
        "    treatment='Side-quest Engagement',    #C\n",
        "    outcome='In-game Purchases',    #C\n",
        "    graph=causal_graph    #D\n",
        ")\n",
        "\n",
        "#A Install DoWhy and load the CausalModel class\n",
        "#B Instantiate the CausalModel object with the data, which represents the level 1 observational distribution from which we derive the estimands.\n",
        "#C We also specify the target causal query we wish to estimate, namely the causal effect of the \"treatment\" on the \"outcome.\"\n",
        "#D And of course we provide the causal DAG."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 11.4: Run identification in DoWhy\n",
        "\n",
        "The `identify_effect` methods show us possible estimands we can target given our causal model and observed variables. The `identified_estimand` object is an object of the class `IdentifiedEstimand`. Printing it will list out the estimands, if any, and the assumptions they entail. In our case we have three estimands we can target:\n",
        "* The backdoor adjustment estimand through the adjustment set Player Skill Level, Guild Membership, and Time Spent Playing\n",
        "* The front-door adjustment estimand through the mediator Won Items\n",
        "* Instrumental variable estimands through Side-quest Group Assignment and Customization Level\n",
        "\n"
      ],
      "metadata": {
        "id": "S-dLarPDiCBU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8TzWYPeqHxk"
      },
      "outputs": [],
      "source": [
        "identified_estimand = model.identify_effect()    #A\n",
        "print(identified_estimand)\n",
        "\n",
        "#A The identify_effect method of the CausalModel class lists identified estimands."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 11.5 and 11.6. Estimating the backdoor estimand with linear regression (and printing the results)\n",
        "\n",
        "In DoWhy, we do estimation using a method in the CausalModel class called `estimate_effect`."
      ],
      "metadata": {
        "id": "1oPk684Fi433"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPn-BWlOsotr"
      },
      "outputs": [],
      "source": [
        "causal_estimate_reg = model.estimate_effect(\n",
        "    identified_estimand,    #A\n",
        "    method_name=\"backdoor.linear_regression\",    #B\n",
        "    confidence_intervals=True    #C\n",
        ")\n",
        "print(causal_estimate_reg)\n",
        "\n",
        "#A The estimate_effect method takes an estimand as input.\n",
        "#B method_name is of the form \"[estimand].[estimator]\". Here we use linear regression estimator to estimate the backdoor estimand.\n",
        "#C Return confidence intervals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnJb-6J2MjR5"
      },
      "source": [
        "The \"Causal Estimate\" is the estimated coefficient for the treatment variable $v_0$.  The p-value is the significance test statistic for the null hypothesis that the true value of the coefficient is 0.  \n",
        "\n",
        "Note that once we have established that we want to use regression as the estimation method, all of the model evaluation methods for regression become relevant.  For example, if there were issues with the fit, such as residuals that had unstable variance or correlated with regression variables, it should cause us to question the linear assumption.  We also care about sparsity.  In this case, we had 30 data points to calculate seven coefficient estimates (one for the $v_0$, one each for the common cause confounders $W$'s and an intercept term.  If there were more potential common causes, we might need more data for this method to work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP38voZAp0cn"
      },
      "source": [
        "## Listing 11.7: Propensity Score Stratification\n",
        "\n",
        "Propensity score methods are a collection of estimation methods for the backdoor estimand that use a quantity called the propensity score. The traditional definition of a propensity score is the probability of being exposed to the treatment conditional on the confounders. In the context of the online gaming example, this is the probability a player has high side-quest engagement given their guild membership (G), time spent playing (T), and player skill level (S), i.e. P(E=1|T=t, G=g, S=s) where t, i, and s are that player’s values for T, I, and S. In other words, it quantifies the player’s “propensity” of being exposed to the treatment (E=1). Typically P(E=1|T=t, G=g, S=s) is fit by logistic regression.\n",
        "But we can take a more expansive, machine learning friendly view of the propensity score. You can learn a propensity score function λ(...) of the backdoor adjustment set of confounders that renders those confounders conditionally independent of the treatment, as in the following figure.\n",
        "\n",
        "![DAG with propensity score node](images/propensity_score_node.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb_5Ny5Lp2XI"
      },
      "outputs": [],
      "source": [
        "causal_estimate_strat = model.estimate_effect(\n",
        "    identified_estimand,\n",
        "    method_name=\"backdoor.propensity_score_stratification\",     #A\n",
        "    target_units=\"ate\",\n",
        "    confidence_intervals=True\n",
        ")\n",
        "\n",
        "print(causal_estimate_strat)\n",
        "\n",
        "#A Propensity score stratification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 11.8: Propensity score matching\n",
        "\n",
        "Propensity score weighting methods use the propensity score to calculate a weight in a class of inference algorithms called inverse probability weighting. We implement this method in DoWhy as follows."
      ],
      "metadata": {
        "id": "mcGNxs5hzaIx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6gIZuKEvaoL"
      },
      "outputs": [],
      "source": [
        "causal_estimate_match = model.estimate_effect(\n",
        "    identified_estimand,\n",
        "    method_name=\"backdoor.propensity_score_matching\",    #A\n",
        "    target_units=\"ate\",\n",
        "    confidence_intervals=True\n",
        ")\n",
        "print(causal_estimate_match)\n",
        "\n",
        "#A Propensity score matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYFIkPMvvZ1l"
      },
      "source": [
        "## Listing 11.9: Propensity score weighting\n",
        "Propensity score weighting methods use the propensity score to calculate a weight in a class of inference algorithms called inverse probability weighting. DoWhy supports [a few different weighting schemes](https://microsoft.github.io/dowhy/dowhy.causal_estimators.html#module-dowhy.causal_estimators.propensity_score_weighting_estimator\n",
        "):\n",
        "1. Vanilla Inverse Propensity Score weighting (IPS) (`weighting_scheme=\"ips_weight\"`)\n",
        "2. Self-normalized IPS weighting (also known as the Hajek estimator) (`weighting_scheme=\"ips_normalized_weight\"`)\n",
        "3. Stabilized IPS weighting (`weighting_scheme = \"ips_stabilized_weight\"`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVXvBf97vtj-"
      },
      "outputs": [],
      "source": [
        "causal_estimate_ipw = model.estimate_effect(\n",
        "    identified_estimand,\n",
        "    method_name=\"backdoor.propensity_score_weighting\",    #A\n",
        "    target_units = \"ate\",\n",
        "    method_params={\"weighting_scheme\":\"ips_weight\"},    #B\n",
        "    confidence_intervals=True\n",
        ")\n",
        "print(causal_estimate_ipw)\n",
        "\n",
        "#A Inverse probability weighting\n",
        "#B Parameters used to set the IPS algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fact that this estimator’s result differs so dramatically from the others suggest that it is relying on statistical assumptions that don’t hold in this data."
      ],
      "metadata": {
        "id": "8lT48_Sn0Mh4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81JeFFJhqkoW"
      },
      "source": [
        "## Listing 11.10: Double ML with DoWhy, EconML, and sklearn\n",
        "\n",
        "Recent developments in causal effect estimation focus on leveraging machine learning models. Most of these target the backdoor estimand. These approaches to causal effect estimation scale to large datasets and allow us to relax parametric assumptions such as linearity. The following DoWhy code uses the sklearn and EconML libraries for these machine learning methods. DoWhy’s `estimate_effects` provides a wrapper to the EconML’s implementation of these methods.\n",
        "\n",
        "Double machine learning is a backdoor estimator that uses machine learning methods to fit two predictive models: a model of the outcome given the adjustment set of confounders and a model of the treatment given the adjustment set. The approach then combines these two predictive models in a final-stage estimation to create a model of the target causal effect query.\n",
        "The following code performs double ML using a gradient boosting model and regularized regression model (LassoCV) from sklearn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvtkvQVtqjoQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gb_estimate = model.estimate_effect(\n",
        "    identified_estimand,\n",
        "    method_name=\"backdoor.econml.dml.DML\",    #A\n",
        "    control_value = 0,\n",
        "    treatment_value = 1,\n",
        "    method_params={\n",
        "        \"init_params\":{\n",
        "            'model_y':GradientBoostingRegressor(),    #B\n",
        "            'model_t': GradientBoostingRegressor(),    #C\n",
        "            \"model_final\":LassoCV(fit_intercept=False),    #D\n",
        "            'featurizer':PolynomialFeatures(degree=1, include_bias=False)\n",
        "        },\n",
        "        \"fit_params\":{}\n",
        "    }\n",
        ")\n",
        "print(gb_estimate)\n",
        "#A Select the double ML estimator\n",
        "#B Use a gradient boosting model to model the outcome given the confounders.\n",
        "#C Use a gradient boosting model to model the treatment given the confounders.\n",
        "#D Use linear regression with L1 regularization (LASSO) as the final model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 11.11: Backdoor estimation with a meta-learner\n",
        "\n",
        "Meta learners are another machine learning method for backdoor estimation. Broadly speaking, meta learners train a model (or models) of the outcome given the treatment variable and the confounders, then account for the difference in prediction across treatment and control values of the treatment variable. They are particularly focused on highlighting heterogeneity of treatment effects across the data. The code below shows a meta learner example called a T learner that uses a random forest predictor.\n"
      ],
      "metadata": {
        "id": "SvBntf_L1T6x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hoVsyTm0aLK"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor    #A\n",
        "metalearner_estimate = model.estimate_effect(    #A\n",
        "    identified_estimand,    #A\n",
        "    method_name=\"backdoor.econml.metalearners.TLearner\",\n",
        "    method_params={    #A\n",
        "        \"init_params\": {'models': RandomForestRegressor()},    #A\n",
        "        \"fit_params\": {}    #A\n",
        "    }    #A\n",
        ")    #A\n",
        "\n",
        "print(metalearner_estimate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-gGZxJ7UotG"
      },
      "source": [
        "## Listing 11.12: Front-door estimation with DoWhy\n",
        "\n",
        "Front-door estimation will run a two-stage linear model estimation procedure targeting the front-door estimand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqNQ3sDIU5Iu"
      },
      "outputs": [],
      "source": [
        "causal_estimate_fd = model.estimate_effect(\n",
        "    identified_estimand,\n",
        "    method_name=\"frontdoor.two_stage_regression\",    #A\n",
        "    target_units = \"ate\",\n",
        "    method_params={\"weighting_scheme\": \"ips_weight\"},    #B\n",
        "    confidence_intervals=True\n",
        ")\n",
        "print(causal_estimate_fd)\n",
        "\n",
        "#A Select two stage regression for the front door estimand.\n",
        "#B Specify estimator hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsLoiBD4vxuW"
      },
      "source": [
        "## 11.13: Instrumental variable estimation and regression discontinuity\n",
        "\n",
        "Instrumental variable (IV) estimation uses the instrument Side-quest Group Assignment to estimate an estimand that doesn't depend on any of the common causes in the estimation.  Whether or not IV estimation is possible depends on the causal DAG.  Like regression estimation, it relies on certain parametric assumptions as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZD_S6u_v1Xl"
      },
      "outputs": [],
      "source": [
        "causal_estimate_iv = model.estimate_effect(\n",
        "    identified_estimand,\n",
        "    method_name=\"iv.instrumental_variable\",    #A\n",
        "    method_params = {\n",
        "        \"iv_instrument_name\": \"Side-quest Group Assignment\"    #B\n",
        "    },\n",
        "    confidence_intervals=True\n",
        ")\n",
        "print(causal_estimate_iv)\n",
        "\n",
        "#A Select intrumental variable estimation.\n",
        "#B Select side-quest engagement as the instrument."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The large confidence interval suggests this estimator has too much variance to be useful."
      ],
      "metadata": {
        "id": "pB_OIrW75Sog"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtPvwg7sv4mo"
      },
      "source": [
        "## Listing 11.14 Regression discontinuity estimation with DoWhy\n",
        "\n",
        "Regression discontinuity is an estimation method popular in econometrics.  Regression discontinuity tries to model the \"do\" intervention by finding a threshold on a continuous variable that partitions the data into two parts corresponding to two different \"do\" values (e.g., \"treatment\" and \"control\").  It compares observations lying closely on either side of the threshold because data points close to that threshold will have similar values for the confounders.\n",
        "\n",
        "DoWhy treats it as a special type of [IV approach](https://microsoft.github.io/dowhy/dowhy.causal_estimators.html#dowhy.causal_estimators.regression_discontinuity_estimator.RegressionDiscontinuityEstimator\n",
        "). The argument `rd_variable_name` name of the variable on where the thresholding occurs. This variable is analogous to the instrument.\n",
        "`rd_threshold_value` is the threshold value where the split (or \"discontinuity\") occurs.  `rd_bandwidth` is the distance from the threshold within which confounders can be considered the same between treatment and control.\n",
        "*italicized text*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC9t3Va7v8yW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "causal_estimate_regdist = model.estimate_effect(\n",
        "    identified_estimand,\n",
        "    method_name=\"iv.regression_discontinuity\",    #A\n",
        "    method_params={\n",
        "        'rd_variable_name':'Customization Level',    #B\n",
        "        'rd_threshold_value':0.5,    #C\n",
        "        'rd_bandwidth': 0.15    #D\n",
        "    },\n",
        "    confidence_intervals=True,\n",
        ")\n",
        "\n",
        "#A DoWhy treats it as a special type of IV estimator.\n",
        "#B Using Customization Level as our instrument.\n",
        "#C The threshold value for the split (\"discontinuity\").\n",
        "#D The distance from the threshold within which confounders are considered the same between treatment and control values of the treatment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Dw3cAq-8Rdi"
      },
      "outputs": [],
      "source": [
        "print(causal_estimate_regdist)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, though the estimate is not too far off the those of the other estimators, the confidence intervals are a bit too large for comfort, suggesting either the instrument is weak or that we need to tune the arguments passed to the estimator.\n"
      ],
      "metadata": {
        "id": "rVCPseen6GOj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYi6yUov1M8q"
      },
      "source": [
        "## Listing 11.15: Refutating the assumption of sufficient data\n",
        "\n",
        "Refutation sensitivity tests enable us to test the sensitivity of our estimations to violations of our model assumptions.\n",
        "\n",
        "### Data size reduction\n",
        "One way to test the robustness of the analysis is to reduce the size of the data and see if you obtain similar results.  If the size of your data small relative to your estimator, then removal of that already sparse data would have an impact on your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WL9wSZVjzS45"
      },
      "outputs": [],
      "source": [
        "identified_estimand.set_identifier_method(\"frontdoor\")    #A\n",
        "res_subset = model.refute_estimate(\n",
        "    identified_estimand,    #B\n",
        "    causal_estimate_fd,    #C\n",
        "    method_name=\"data_subset_refuter\",    #D\n",
        "    subset_fraction=0.8,    #E\n",
        "    num_simulations=100    #F\n",
        ")\n",
        "print(res_subset)\n",
        "\n",
        "#A Not strictly necessary, but in some cases clarifying the estimand when working with refuters helps avoid errors.\n",
        "#B The refute_estimate function takes in the identified estimand...\n",
        "#C ... and the estimator that targets the estimand.\n",
        "#D We select \"data_subset_refuter\", which tests if the causal estimate is different when we run the analysis on a subset of the data.\n",
        "#E Set the size of the subset to 80% the size of the original data.\n",
        "#F Set the number of simulations to 30."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96fO49Pt2h3P"
      },
      "source": [
        "“Estimand Effect” is the effect from our original analysis.  “New effect” is the average ATE across the simulations.  We hope these two effects to be similar.  The p-value is calculated under the null hypothesis that the two are the same.  P-values falling below a traditional significance threshold (e.g., .1, .05) indicate evidence that the two effects are different, which would mean our analysis is sensitive to the size of our data.  This is not the case with our analysis, since our p-value is high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAWEyzzM2H2B"
      },
      "source": [
        "## Listing 11.16: Adding a dummy variable\n",
        "\n",
        "One way to test our models is to add dummy variables – variables that have no causal bearing on our problem, but where the additional noise can impact the performance of the statistical estimator.  One way to test this is by adding a dummy confounder and re-running the analysis.  The dummy confounder is a random variable that is not actually a confounder, it is completely independent of the treatment and outcome variables, but the analysis treats it as a confounder.  By doing this many times, we can evaluate how sensitive your results are to the random common cause."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zyNBy4v2Bwo"
      },
      "outputs": [],
      "source": [
        "identified_estimand.set_identifier_method(\"backdoor\")\n",
        "\n",
        "res_random = model.refute_estimate(    #A\n",
        "    identified_estimand,    #A\n",
        "    gb_estimate,    #A\n",
        "    method_name=\"random_common_cause\",    #A\n",
        "    num_simulations=100,    #A\n",
        "    n_jobs=2    #A\n",
        ")    #A\n",
        "print(res_random)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRZZKH9p2oDU"
      },
      "source": [
        "“Estimated effect” is the original causal effect estimate that we obtained from your model before running the refutation test.  “New effect” is the new causal effect estimate obtained after adding a random common cause to your data and re-running the analysis.  “p value” is the p-value of the test comparing the original and new estimated effects. A small p-value suggests that the two effects are statistically different, indicating that your original estimate might be sensitive to potential unobserved confounding.\n",
        "Given ourresults, the original estimated effect and the new effect after adding a random common cause are quite similar, and the p-value is greater than typical significance cutoffs (e.g., 0. or 0.05). This means there's no strong statistical evidence to suggest that the estimated effect changes significantly with the addition of a random common cause. So, the original estimated causal effect appears to be quite robust, at least to the addition of random common causes, according to this specific refutation test."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 11:17: Replacing the treatment variable with a dummy variable\n",
        "\n",
        "We can also experiment with replacing the treatment variable with a dummy (placebo) variable.  Here, we expect the ATE to be close to 0."
      ],
      "metadata": {
        "id": "_rstWy7V6upL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqCyNguG2zCj"
      },
      "outputs": [],
      "source": [
        "identified_estimand.set_identifier_method(\"backdoor\")\n",
        "\n",
        "res_placebo = model.refute_estimate(\n",
        "    identified_estimand,    #A\n",
        "    causal_estimate_ipw,    #A\n",
        "    method_name=\"placebo_treatment_refuter\",    #A\n",
        "    placebo_type=\"permute\",    #A\n",
        "    num_simulations=100    #A,\n",
        ")\n",
        "\n",
        "print(res_placebo)\n",
        "\n",
        "#A This refuter replaces the treatment variable with a dummy (placebo) variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eIf2rCn3PZU"
      },
      "source": [
        "“New Effect” is the ATE generated when the treatment variable (in our case Tech Support) is replaced in the analysis by a random dummy variable.  We expect the new ATE to be 0, and the p-value here compares New Effect value to 0.  Values that are significantly different from 0 will have a low p-value.  When the p-value falls below typical significance thresholds (e.g., .05), the test shows evidence that your analysis might be overly sensitive and that you should be suspicious of your ATE estimates.  In our case, the high p-value suggests that this is not the case.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 11.18: Replacing the outcome variable with a dummy variable\n",
        "\n",
        "We can substitute the outcome variable with a dummy variable. The ATE in this case should be 0 because the treatment has no effect on this dummy. We’ll simulate it as a linear function of some of the confounders so the outcome still has a meaningful relationship with some of the covariates. I’ll try this with the front door estimator."
      ],
      "metadata": {
        "id": "5PfiXOTF7IYU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mms5qfR63SpC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "coefficients = np.array([100.0, 50.0])\n",
        "bias = 50.0\n",
        "def linear_gen(df):     #A\n",
        "    subset = df[['guild_membership','player_skill_level']]     #A\n",
        "    y_new = np.dot(subset.values, coefficients) + bias     #A\n",
        "    return y_new     #A\n",
        "\n",
        "identified_estimand.set_identifier_method(\"frontdoor\")\n",
        "ref = model.refute_estimate(    #B\n",
        "    identified_estimand,    #B\n",
        "    causal_estimate_fd,    #B\n",
        "    method_name=\"dummy_outcome_refuter\",    #B\n",
        "    outcome_function=linear_gen    #B\n",
        ")    #B\n",
        "\n",
        "res_dummy_outcome = ref[0]\n",
        "print(res_dummy_outcome)\n",
        "\n",
        "#A Create a function that generates a new dummy outcome variable as a linear function of the covariates.\n",
        "#B Runs refute_estimate with a dummy outcome refuter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, the p-value is calculated under the null hypothesis that New effect equals 0, and a low p-value refutes our assumptions. In this case, our assumptions are not refuted.\n"
      ],
      "metadata": {
        "id": "Stidih4h7ruC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmtk16EI4cGr"
      },
      "source": [
        "## Listing 11.19: Adding an unobserved confounder\n",
        "\n",
        "Since we used backdoor adjustment, we assume that the adjustment set blocks all backdoor paths.  If there were an unobserved confounder that we failed to adjust for, that assumption is violated and our estimate would have confounder bias.  That is not necessarily the worst thing; if we adjust for all major confounders, any bias from unobserved confounders could be small and not impact our results by much.  On the other hand, missing a major confounder could lead us to conclude there is a nonzero ATE when there is none, or conclude a positive ATE when the true ATE is negative, or vice versa.  We therefore test our robust our analysis is to introducing unobserved confounders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqTWKg1f3ukC"
      },
      "outputs": [],
      "source": [
        "identified_estimand.set_identifier_method(\"backdoor\")\n",
        "res_unobserved = model.refute_estimate(    #A\n",
        "    identified_estimand,    #A\n",
        "    causal_estimate_strat,    #A\n",
        "    method_name=\"add_unobserved_common_cause\"    #A\n",
        ")    #A\n",
        "\n",
        "print(res_unobserved)\n",
        "\n",
        "#A Setting up a refuter that adds an unobserved common cause"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This analysis does not return a p-value. It produces the above heat map, which shows how quickly the estimate changes when the no unobserved confounder assumption is violated.  The horizontal axis in the heat map shows the various levels of influence the unobserved confounder has on the outcome, and the vertical axis shows the various levels of influence the confounder can have on the treatment. The color corresponds to the new effect estimates that result at different levels of influence. Here, we see the ATE is quite sensitive to the effect the confounder has on the treatment. Note that you can change the default parameters of the refuter to experiment with different impacts the confounder could have on the treatment and outcome."
      ],
      "metadata": {
        "id": "wBSKEYR379S4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}