{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiaLfEpZRPC9",
        "outputId": "97de1690-3201-48a1-a527-af0a2afe0e57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.10/dist-packages (1.8.6)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl) (3.3.0)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl) (0.1.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl) (4.66.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pyro-ppl) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pyro-ppl) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pyro-ppl) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pyro-ppl) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pyro-ppl) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pyro-ppl) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pyro-ppl) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pyro-ppl) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pyro-ppl) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyro-ppl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load a dsprites image"
      ],
      "metadata": {
        "id": "3Ta_VCR3ifvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io    #A\n",
        "import urllib.request    #A\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np    #A\n",
        "import torch\n",
        "\n",
        "url = 'https://github.com/altdeep/causalML/blob/master/book/chapter%208/sprites_example.npz?raw=true'    #A\n",
        "with urllib.request.urlopen(url) as response:    #A\n",
        "    data = response.read()    #A\n",
        "file = io.BytesIO(data)    #A\n",
        "npzfile = np.load(file)    #A\n",
        "img_dict = dict(npzfile)    #A\n",
        "img = torch.tensor(img_dict['image'].astype(np.float32) )   #B\n",
        "plt.imshow(img, cmap='Greys_r', interpolation='nearest')    #B\n",
        "plt.axis('off')    #B\n",
        "plt.title('original')    #B\n",
        "plt.show()    #B\n",
        "causal_factor = torch.from_numpy(img_dict['label']).unsqueeze(0)    #C\n",
        "print(causal_factor)    #C\n",
        "\n",
        "#A Download sprites example from GitHub and load it.\n",
        "#B Download the sprites image.\n",
        "#C The causal factors of the example is [0  0  1 13 26 14], the first element is always 0, the second element corresponds to \"square\" and is represented by 0. The remaining elements correspond to scale, orientation, and X and Y positions."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "jgtzoTkERcY_",
        "outputId": "b0092e41-5b69-4e34-db29-6059d86ac727"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  0,  1, 13, 26, 14]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the encoder of causal factors"
      ],
      "metadata": {
        "id": "GBFa6olSoG2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import torch.nn as nn\n",
        "\n",
        "class EncoderCausalFactors(nn.Module):    #A\n",
        "  def __init__(self, image_dim, factor_dim):\n",
        "    super(EncoderCausalFactors, self).__init__()\n",
        "    self.image_dim = image_dim\n",
        "    self.factor_dim = factor_dim\n",
        "    hidden_dim = 1000    #B\n",
        "    self.fc1 = nn.Linear(image_dim, hidden_dim)    #C\n",
        "    self.fc2 = nn.Linear(hidden_dim, hidden_dim)    #C\n",
        "    self.fc3 = nn.Linear(hidden_dim, factor_dim)    #C\n",
        "    self.softplus = nn.Softplus()    #C\n",
        "    self.sigmoid = nn.Sigmoid()    #D\n",
        "\n",
        "  def forward(self, img):\n",
        "    img = img.reshape(-1, self.image_dim)    #E\n",
        "    hidden1 = self.softplus(self.fc1(img))    #F\n",
        "    hidden2 = self.softplus(self.fc2(hidden1))    #F\n",
        "    p_loc = self.sigmoid(self.fc3(hidden2))    #G\n",
        "    return p_loc    #G\n",
        "\n",
        "encoder_n_causal_factors = EncoderCausalFactors(    #H\n",
        "    image_dim=64*64,    #H\n",
        "    factor_dim=sum([1,3,6,40,32,32])    #H\n",
        ")\n",
        "\n",
        "url = 'https://github.com/altdeep/causalML/raw/master/book/chapter%208/sprites-model-encoder-causal-factors.pt'    #I\n",
        "response = requests.get(url)    #I\n",
        "response.raise_for_status()    #I\n",
        "with open('temp_weights.pt', 'wb') as f:    #I\n",
        "    f.write(response.content)    #I\n",
        "state_dict = torch.load(    #I\n",
        "    'temp_weights.pt',    #I\n",
        "    map_location=torch.device('cpu')    #I\n",
        ")    #I\n",
        "encoder_n_causal_factors.load_state_dict(state_dict)    #I\n",
        "#A Encoder for the the vector of exogenous parents of the causal factors.\n",
        "#B The hidden layers have a length of 1000\n",
        "#C Using a linear transforms passed through softplus activation functions.\n",
        "#D The final activation is a sigmoid function.\n",
        "#E Flatten the image.\n",
        "#F Calculate the hidden layers.\n",
        "#G The output layer generates a probability vector passed OneHotCategorical distribution.\n",
        "#H Initialize the encoder. The image dimension is 64x64 pixels, the six elements of the causal factor vector are one-hot encoded into a vector of length 1+3+6+40+32+32=114\n",
        "#I Download the pre-trained weights to a temporary file and load them into the model.  You can download the weights from 'https://github.com/altdeep/causalML/blob/master/book/chapter%208/sprites-model-encoder-causal-factors.pt?raw=true'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JmAKbB3SXu2",
        "outputId": "2886c527-6265-464e-b061-f031c61d0b2a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate examples of causal exogenous factors"
      ],
      "metadata": {
        "id": "YRGWpn-ZoO-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyro import distributions as dist\n",
        "\n",
        "CARDINALITY = [1, 3, 6, 40, 32, 32]    #A\n",
        "\n",
        "def decode_one_hot(factor_encoded, cardinality=CARDINALITY):    #B\n",
        "    split = [    #B\n",
        "        torch.split(element, cardinality)     #B\n",
        "        for element in factor_encoded    #B\n",
        "    ]    #B\n",
        "    labels = [[int(torch.argmax(vec)) for vec in item] for item in split] #B\n",
        "    return torch.tensor(labels)    #B\n",
        "\n",
        "def sample_one_hot(p_encoded, cardinality=CARDINALITY):    #C\n",
        "    split = [torch.split(element, cardinality) for element in p_encoded]    #C\n",
        "    sample_list = [    #C\n",
        "        [    #C\n",
        "            dist.OneHotCategorical(p_vec).sample()    #C\n",
        "            for p_vec in item    #C\n",
        "        ] for item in split    #C\n",
        "    ]    #C\n",
        "    sample = torch.stack([    #C\n",
        "        torch.cat(samples, -1)    #C\n",
        "        for samples in sample_list    #C\n",
        "    ])    #C\n",
        "    return sample    #C\n",
        "\n",
        "inferred_cause_p = encoder_n_causal_factors.forward(img)    #D\n",
        "sampled_factors = sample_one_hot(    #D\n",
        "    inferred_cause_p    #D\n",
        ")    #D\n",
        "print(decode_one_hot(sampled_factors))    #D\n",
        "\n",
        "#A Set the cardinality of the causal factor vector.\n",
        "#B Helper function that decodes the one-hot encoded output of the encoder.\n",
        "#C Samples from the output probability vector of the encoder_causal_factors.\n",
        "#D Use encoder to predict causal factors."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHTVvR7ZTbUB",
        "outputId": "b74593f9-8a8f-4673-d54e-914c47475d52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  0,  1, 13, 26, 14]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## An encoder for inference of $N_I$"
      ],
      "metadata": {
        "id": "DBq-FfSQYHVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderNImage(nn.Module):    #A\n",
        "  def __init__(self, image_dim, factor_dim, n_image_dim):\n",
        "    super(EncoderNImage, self).__init__()\n",
        "    self.image_dim = image_dim\n",
        "    self.factor_dim = factor_dim\n",
        "    self.n_image_dim = n_image_dim\n",
        "    hidden_dim = 1000\n",
        "    self.fc1 = nn.Linear(\n",
        "        self.image_dim + self.factor_dim, hidden_dim    #B\n",
        "    )    #B\n",
        "    self.fc2 = nn.Linear(hidden_dim, hidden_dim)    #B\n",
        "    self.fc31 = nn.Linear(hidden_dim, n_image_dim)    #B\n",
        "    self.fc32 = nn.Linear(hidden_dim, n_image_dim)    #B\n",
        "    self.softplus = nn.Softplus()    #B\n",
        "\n",
        "  def forward(self, img, factor):\n",
        "    img = img.reshape(-1, self.image_dim)    #C\n",
        "    inputs = torch.cat((img, factor), -1)    #D\n",
        "    hidden1 = self.softplus(self.fc1(inputs))    #E\n",
        "    hidden2 = self.softplus(self.fc2(hidden1))    #E\n",
        "    n_image_loc = self.fc31(hidden2)    #F\n",
        "    n_image_scale = torch.exp(self.fc32(hidden2))    #F\n",
        "    return n_image_loc, n_image_scale    #F\n",
        "\n",
        "def encode_one_hot(factor, cardinality=CARDINALITY):    #G\n",
        "    new_factor = []    #G\n",
        "    for i, factor_length in enumerate(cardinality):    #G\n",
        "        new_factor.append(    #G\n",
        "            torch.nn.functional.one_hot(    #G\n",
        "                factor[:,i].to(torch.int64), int(factor_length)    #G\n",
        "            )    #G\n",
        "        )    #G\n",
        "    new_factor = torch.cat(new_factor, -1)    #G\n",
        "    return new_factor.to(torch.float32)    #G\n",
        "\n",
        "encoder_n_image = EncoderNImage(    #H\n",
        "    image_dim=64*64,    #H\n",
        "    factor_dim=sum(CARDINALITY),    #H\n",
        "    n_image_dim=50    #H\n",
        ")    #H\n",
        "url = \"https://github.com/altdeep/causalML/raw/master/book/chapter%208/sprites-model-encoder-n-image.pt\"    #I\n",
        "response = requests.get(url)    #I\n",
        "response.raise_for_status()    #I\n",
        "with open('temp_weights.pt', 'wb') as f:    #I\n",
        "    f.write(response.content)    #I\n",
        "state_dict = torch.load(    #I\n",
        "    'temp_weights.pt',    #I\n",
        "    map_location=torch.device('cpu')    #I\n",
        ")    #I\n",
        "encoder_n_image.load_state_dict(state_dict)    #I\n",
        "\n",
        "n_image_loc, n_image_scale = encoder_n_image.forward(    #J\n",
        "    img,    #J\n",
        "    encode_one_hot(causal_factor)    #J\n",
        ")    #J\n",
        "n_image = torch.normal(n_image_loc, n_image_scale)    #K\n",
        "\n",
        "#A Encoder for NI, which serves as both the exogenous variable for the image in causal terms, and the encoding of the image in VAE terms.\n",
        "#B Using a linear transforms passed into a softplus activation function.\n",
        "#C Flatten the image.\n",
        "#D Concatenate the image and the causal factor vector.\n",
        "#E Calculate the hidden layers.\n",
        "#F Calculate the location and scale parameter of multivariate normal distribution on N_image.\n",
        "#G A helper function one-hot encoding a factor.\n",
        "#H Initialize the encoder.\n",
        "#H Load the pretrained weights.  The weights are available from https://github.com/altdeep/causalML/blob/master/book/chapter%208/sprites-model-encoder-n-image.pt\n",
        "#I pass the image and causal factors into the encoder and obtain NI location and scale parameters\n",
        "#J Generate from the posterior distribution on NI."
      ],
      "metadata": {
        "id": "BEJjbHjDUUt3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and run the decoder that maps causes and $N_I$ to images"
      ],
      "metadata": {
        "id": "mwBuVHiLYdZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):    #A\n",
        "  def __init__(self, image_dim, factor_dim, n_image_dim):\n",
        "    super(Decoder, self).__init__()\n",
        "    hidden_dim = 1000\n",
        "    self.fc1 = nn.Linear(n_image_dim + factor_dim, hidden_dim)    #B\n",
        "    self.fc2 = nn.Linear(hidden_dim, hidden_dim)    #B\n",
        "    self.fc3 = nn.Linear(hidden_dim, hidden_dim)    #B\n",
        "    self.fc4 = nn.Linear(hidden_dim, image_dim)    #B\n",
        "    self.softplus = nn.Softplus()    #B\n",
        "    self.sigmoid = nn.Sigmoid()    #B\n",
        "\n",
        "  def forward(self, n_image, factor):\n",
        "    inputs = torch.cat((n_image, factor), -1)    #C\n",
        "    hidden1 = self.softplus(self.fc1(inputs))    #D\n",
        "    hidden2 = self.softplus(self.fc2(hidden1))    #D\n",
        "    hidden3 = self.softplus(self.fc3(hidden2))    #D\n",
        "    p_img = self.sigmoid(self.fc4(hidden3))    #E\n",
        "    return p_img    #E\n",
        "\n",
        "decoder = Decoder(    #F\n",
        "    image_dim=64*64,    #F\n",
        "    factor_dim=sum(CARDINALITY),    #F\n",
        "    n_image_dim=50    #F\n",
        ")    #F\n",
        "\n",
        "url = \"https://github.com/altdeep/causalML/raw/master/book/chapter%208/sprites-model-decoder.pt\"    #G\n",
        "response = requests.get(url)    #G\n",
        "response.raise_for_status()    #G\n",
        "with open('temp_weights.pt', 'wb') as f:    #G\n",
        "    f.write(response.content)    #G\n",
        "state_dict = torch.load(    #G\n",
        "    'temp_weights.pt',    #G\n",
        "    map_location=torch.device('cpu')    #G\n",
        ")    #G\n",
        "decoder.load_state_dict(state_dict)    #G\n",
        "\n",
        "#A The decoder maps from causal factors and N_image to generate a parameter for a multivariate Bernoulli distribution on images.\n",
        "#B The model uses linear transforms, a softplus activate for hidden layers, and sigmoid activate on the output layer.\n",
        "#C The network concatenates n_image and factors in the input layer.\n",
        "#D The input is passed through three hidden layers with softplus activation functions.\n",
        "#E The output is a probability parameter passed to a multivariate Bernoulli distribution on image pixels.\n",
        "#F Initialize the decoder.\n",
        "#G Load the pretrained weights.  The weights are available at https://github.com/altdeep/causalML/blob/master/book/chapter%208/sprites-model-decoder.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqY_qNX0YURx",
        "outputId": "c3020c77-b557-492f-e022-36e5d90947cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper function for plotting counterfactual image"
      ],
      "metadata": {
        "id": "slU9qiPcILR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_reconstruction(original, generated):    #A\n",
        "    fig = plt.figure()    #A\n",
        "    ax0 = fig.add_subplot(121)    #A\n",
        "    plt.imshow(    #A\n",
        "        original.cpu().reshape(64, 64),    #A\n",
        "        cmap='Greys_r',    #A\n",
        "        interpolation='nearest'    #A\n",
        "    )    #A\n",
        "    plt.axis('off')    #A\n",
        "    plt.title('actual')    #A\n",
        "    ax1 = fig.add_subplot(122)    #A\n",
        "    plt.imshow(    #A\n",
        "        generated.reshape(64, 64),    #A\n",
        "        cmap='Greys_r', interpolation='nearest')    #A\n",
        "    plt.axis('off')    #A\n",
        "    plt.title('counterfactual')    #A\n",
        "    plt.show()    #A\n",
        "\n",
        "#A The help function plots the original and counterfactual image side-by-side"
      ],
      "metadata": {
        "id": "Gstav1yOI_lW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create exogenous distribution and assignment function for the image"
      ],
      "metadata": {
        "id": "cxa_GKfMpj8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def p_n_image(n_image_params):    #A\n",
        "    n_image_loc, n_image_scale, n_unif_upper = n_image_params    #B\n",
        "    n_image_norm = dist.Normal(n_image_loc, n_image_scale).to_event(1).sample()    #C\n",
        "    n_image_unif = dist.Uniform(0, n_unif_upper).expand(    #D\n",
        "        torch.Size([1, 64*64])    #D\n",
        "    ).sample()    #D\n",
        "    n_image = n_image_norm, n_image_unif    #E\n",
        "    return n_image\n",
        "\n",
        "def f_image(factor, n_image):    #F\n",
        "    n_image_norm, n_image_unif = n_image    #G\n",
        "    p_output = decoder.forward(    #H\n",
        "        n_image_norm,    #H\n",
        "        encode_one_hot(factor)    #H\n",
        "    )    #H\n",
        "    sim_img = (n_image_unif <= p_output).int()    #I\n",
        "    return sim_img\n",
        "\n",
        "#A A function that generates a variate from the N_image exogenous distribution.\n",
        "#B The parameters the N_image's distribution include location and scale parameter for a normal distribution, and the upper bound of a uniform distribution.\n",
        "#C Sample a normal random variate from the Normal distribution.\n",
        "#D Sample a uniform random variate from a Uniform distribution.\n",
        "#E We combine these into a single n_image object.\n",
        "#F Assignment function for the image.\n",
        "#G The exogenous noise variable decomposes into a normal random variable and a uniform random variate.\n",
        "#H the normal random variable is passed through the decoder to get a probability vector for the pixels.\n",
        "#I Each pixel is set deterministically with an indicator function that returns 1 if an element of the uniform variate is less than the corresponding element of the probability vector, or otherwise returns 0."
      ],
      "metadata": {
        "id": "5ZG_-qWfpxWA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate a counterfactual image"
      ],
      "metadata": {
        "id": "5zVIuX0wZPnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def abduct(img, factor, smoother=1e-3):    #A\n",
        "    n_image_loc, n_image_scale = encoder_n_image.forward(    #B\n",
        "        img, encode_one_hot(factor)    #B\n",
        "    )    #B\n",
        "    n_unif_upper = decoder.forward(    #C\n",
        "        n_image_loc,    #C\n",
        "        encode_one_hot(factor)    #C\n",
        "    )    #C\n",
        "    n_unif_upper = n_unif_upper * (1 - 2 * smoother) + smoother    #C\n",
        "    p_image_params = n_image_loc, n_image_scale, n_unif_upper    #D\n",
        "    return p_image_params\n",
        "\n",
        "def do_action(factor, element=1, val=2):    #E\n",
        "    intervened_factor = factor.clone()    #E\n",
        "    intervened_factor[0][element] = val    #E\n",
        "    return intervened_factor    #E\n",
        "\n",
        "def predict(intervened_factor, n_image_params):    #F\n",
        "    n_image = p_n_image(n_image_params)    #F\n",
        "    sim_img = f_image(intervened_factor, n_image)    #F\n",
        "    return sim_img    #F\n",
        "\n",
        "def counterfactual(img, factor):    #G\n",
        "    p_image_params = abduct(img, factor)    #G\n",
        "    intervened_factor = do_action(factor)    #G\n",
        "    pred_recon = predict(intervened_factor, p_image_params)    #G\n",
        "    compare_reconstruction(img, pred_recon)    #G\n",
        "\n",
        "counterfactual(img, causal_factor)    #H\n",
        "\n",
        "#A Abduction step: Infer the exogenous variable given the image.\n",
        "#B Infer the parameters of N_I. First, this includes two parameters of a normal distribution.\n",
        "#C Secondly, we infer the upper bound of a uniform distribution and apply smoothing so it is not exactly 1 or 0.\n",
        "#D Combine these together into one inferred parameter set.\n",
        "#E Action step: Apply the intervention that sets the shape element to \"heart\" (represented by the integer 2).\n",
        "#F Prediction step: Generate n_image from P(N_image), pass this through assignment function to generate an image.\n",
        "#G Apply all three steps, \"abduct\" the n_image, apply the intervention, forward generate the counterfactual image.\n",
        "#H Plot the result."
      ],
      "metadata": {
        "id": "_tS1LwhRVx2w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "a184d570-b633-43dc-b638-fb2c7cdd081b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}