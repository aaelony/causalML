{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1ispDL1G4Fc"
      },
      "source": [
        "# Chapter 5 - Connecting Causality and Deep Learning\n",
        "\n",
        "The notebook is a code companion to chapter 5 of the book [Causal AI](https://www.manning.com/books/causal-ai) by [Robert Osazuwa Ness](https://www.linkedin.com/in/osazuwa/).\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/altdeep/causalML/blob/master/book/chapter%205/chapter_5_Connecting_Causality_and_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This notebook was written in Google Colab using Python version 3.10.12. The versions of the main libraries include:\n",
        "* pyro version 1.84\n",
        "* torch version 2.2.1\n",
        "* pandas version 2.0.3\n",
        "* torchvision vserions 0.18.0+cu121\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pgmpy allows us to fit conventional Bayesian networks on a causal DAG. However, with modern deep probabilistic machine learning frameworks like pyro, we can build more nuanced and powerful causal models.  In this tutorial, we fit a variational autoencoder on a causal DAG that represents a dataset that mixes handwritten MNIST digits and typed T-MNIST images.\n",
        "\n",
        "![TMNIST-MNIST](https://github.com/altdeep/causalML/blob/master/book/chapter%205/images/MNIST-TMNIST.png?raw=1)"
      ],
      "metadata": {
        "id": "UVk-jq9hxOYo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_gEnU8rWN3e"
      },
      "outputs": [],
      "source": [
        "!pip install pyro-ppl==1.8.4\n",
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 5.1: Setup for GPU training\n",
        "\n",
        "The code will run faster if we use CUDA, if it's available."
      ],
      "metadata": {
        "id": "Rsu5QxOW0aAx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfWTRtPoWF5H"
      },
      "outputs": [],
      "source": [
        "import torch    #A\n",
        "USE_CUDA = False    #A\n",
        "DEVICE_TYPE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")    #A\n",
        "#A Use CUDA if it is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UoHQdJpG4Fe"
      },
      "source": [
        "## Listing 5.2: Downloading, combining and loading the data\n",
        "\n",
        "First, we download the data and combine it into a Dataset object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GoP_qMKWF5I"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import transforms\n",
        "\n",
        "class CombinedDataset(Dataset):    #A\n",
        "    def __init__(self, csv_file):\n",
        "        self.dataset = pd.read_csv(csv_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        images = self.dataset.iloc[idx, 3:]    #B\n",
        "        images = np.array(images, dtype='float32')/255.  #B\n",
        "        images = images.reshape(28, 28)    #B\n",
        "        transform = transforms.ToTensor()    #B\n",
        "        images = transform(images)    #B\n",
        "        digits = self.dataset.iloc[idx, 2]    #C\n",
        "        digits = np.array([digits], dtype='int')    #C\n",
        "        is_handwritten = self.dataset.iloc[idx, 1]    #D\n",
        "        is_handwritten = np.array([is_handwritten], dtype='float32')    #D\n",
        "        return images, digits, is_handwritten    #E\n",
        "\n",
        "def setup_dataloaders(batch_size=64, use_cuda=USE_CUDA):    #F\n",
        "    combined_dataset = CombinedDataset(\n",
        "\"https://raw.githubusercontent.com/altdeep/causalML/master/datasets/combined_mnist_tmnist_data.csv\"\n",
        "    )\n",
        "    n = len(combined_dataset)\n",
        "    train_size = int(0.8 * n)\n",
        "    test_size = n - train_size\n",
        "    train_dataset, test_dataset = random_split(\n",
        "        combined_dataset,\n",
        "        [train_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        **kwargs\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        **kwargs\n",
        "    )\n",
        "    return train_loader, test_loader\n",
        "#A This class loads and processes a dataset that combines the MNIST and Typeface MNIST. The output is a torch.utils.data.Dataset object.\n",
        "#B Load, normalize, and reshape the images to a 28x28 pixel.\n",
        "#C Get and process the digits labels, 0-9.\n",
        "#D 1 for handwritten digits (MNIST) 0 for “typed’ digits (TMNIST)\n",
        "#E Return tuple of the image, the digit label, and the is_handwritten label\n",
        "#F Setup data loader that loads the data and splits it into training and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 5.3: Implement the decoder\n",
        "\n",
        "First, we specify a decoder. The decoder maps the latent variable Z, a variable representing the value of the digit, and a binary variable representing whether the digit is handwritten."
      ],
      "metadata": {
        "id": "fRXUclDPpKpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class Decoder(nn.Module):    #A\n",
        "    def __init__(self, z_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        img_dim = 28 * 28    #B\n",
        "        digit_dim = 10    #C\n",
        "        is_handwritten_dim = 1    #D\n",
        "        self.softplus = nn.Softplus()    #E\n",
        "        self.sigmoid = nn.Sigmoid()    #E\n",
        "        self.fc1 = nn.Linear(z_dim + digit_dim + is_handwritten_dim, hidden_dim)    #F\n",
        "        self.fc2 = nn.Linear(hidden_dim, img_dim)    #G\n",
        "\n",
        "    def forward(self, z, digit, is_handwritten):    #H\n",
        "        input = torch.cat([z, digit, is_handwritten], dim=1)   #I\n",
        "        hidden = self.softplus(self.fc1(input))    #J\n",
        "        img_param = self.sigmoid(self.fc2(hidden))    #K\n",
        "        return img_param\n",
        "#A The decoder method of a VAE class.\n",
        "#B Image is 28 by 28 pixels\n",
        "#C Digit is one-hot encoded digits 0-9, i.e., a vector of length 10.\n",
        "#D An indicator for if the digit is handwritten that has size 1\n",
        "#E The softplus and sigmoid are nonlinear transforms (activation functions) used in mapping between layers.\n",
        "#F fc1 is a linear function that maps Z vector, the digit, and the is_handwritten to a linear out, which is passed through a softplus activation function to create a \"hidden layer\" - a vector whose length is given by hidden_layer.\n",
        "#G The fc2 linearly maps the hidden layer to an output passed to a sigmoid function. The resulting value is a value between 0 and 1.\n",
        "#H Define the forward computation from the latent Z variable value to a generated X variable value.\n",
        "#I First combine Z and the labels.\n",
        "#J Then compute the hidden layer.\n",
        "#K Finally, pass the hidden layer to a linear transform, then to a sigmoid transform to output a parameter vector of length 784. Each element of the vector corresponds to a Bernoulli parameter value for an image pixel.\n"
      ],
      "metadata": {
        "id": "pQEwj1mh-TTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 5.4: The causal model\n",
        "\n",
        "The `model` method implements the causal model. First it samples the latent variable Z, the digit variable, and the is_handwritten variable. These are passed to the decoder, which generates the image."
      ],
      "metadata": {
        "id": "aYlhi-IlpshP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "\n",
        "dist.enable_validation(False)    #A\n",
        "def model(self, data_size=1):    #B\n",
        "    pyro.module(\"decoder\", self.decoder)    #B\n",
        "    options = dict(dtype=torch.float32, device=DEVICE_TYPE)\n",
        "    z_loc = torch.zeros(data_size, self.z_dim, **options)    #C\n",
        "    z_scale = torch.ones(data_size, self.z_dim, **options)    #C\n",
        "    z = pyro.sample(\"Z\", dist.Normal(z_loc, z_scale).to_event(1))    #C\n",
        "    p_digit = torch.ones(data_size, 10, **options)/10    #D\n",
        "    digit = pyro.sample(    #D\n",
        "        \"digit\",    #D\n",
        "        dist.OneHotCategorical(p_digit)    #D\n",
        "    )    #D\n",
        "    p_is_handwritten = torch.ones(data_size, 1, **options)/2    #E\n",
        "    is_handwritten = pyro.sample(    #E\n",
        "        \"is_handwritten\",    #E\n",
        "        dist.Bernoulli(p_is_handwritten).to_event(1)    #E\n",
        "    )    #E\n",
        "    img_param = self.decoder(z, digit, is_handwritten)    #F\n",
        "    img = pyro.sample(\"img\", dist.Bernoulli(img_param).to_event(1))  #G\n",
        "    return img, digit, is_handwritten\n",
        "#A Disabling distribution validation lets Pyro calculate loglikelihoods for pixels even though the pixels are not binary values.\n",
        "#B The model of a single image. Within the method we register the decoder, a PyTorch module, with Pyro. This lets Pyro know about the parameters inside of the decoder network.\n",
        "#C We model the joint probability of Z, digit, and is_handwritten sampling each from canonical distributions. We sample Z from a multivariate normal with location parameter z_loc (all zeros) and scale parameter z_scale (all ones).\n",
        "#D We also sample the digit from a one-hot categorical distribution. Equal probability is assigned to each digit.\n",
        "#E We similarly sample the is_handwritten variable from a Bernoulli.\n",
        "#F The decoder maps digit, is_handwritten, and Z to a probability parameter vector.\n",
        "#G That parameter vector is passed to the Bernoulli distribution, which models the pixel values in the data. The pixels are not technically Bernoulli binary variables, but we'll relax this assumption."
      ],
      "metadata": {
        "id": "ndMbxPUfJlqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 5.5 Method for applying model to N images in data\n",
        "\n",
        "`training_model` extends `model` towards representing each image in the dataset."
      ],
      "metadata": {
        "id": "49xn77L8qCmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_model(self, img, digit, is_handwritten, batch_size):    #A\n",
        "    conditioned_on_data = pyro.condition(    #B\n",
        "        self.model,\n",
        "        data={\n",
        "            \"digit\": digit,\n",
        "            \"is_handwritten\": is_handwritten,\n",
        "            \"img\": img\n",
        "        }\n",
        "    )\n",
        "    with pyro.plate(\"data\", batch_size):    #C\n",
        "        img, digit, is_handwritten = conditioned_on_data(batch_size)\n",
        "    return img, digit, is_handwritten\n",
        "#A The model represents the data generating process for one image. The training_model applies that model to the N images in the training data.\n",
        "#B Now we condition the model on the evidence in the training data.\n",
        "#C This context manager represents the N-size plate representing repeating IID examples in the data in Figure 5.9. In this case, N is the batch size. It works like a for loop iterating over each data unit in the batch."
      ],
      "metadata": {
        "id": "RQGgpOPSWOKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 5.6: Implement the encoder\n",
        "\n",
        "The encoder takes an image, the digit, and whether the variable is handwritten, and infers the latent representation Z."
      ],
      "metadata": {
        "id": "ia-xBsT6qioA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):    #A\n",
        "    def __init__(self, z_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        img_dim = 28 * 28    #B\n",
        "        digit_dim = 10    #C\n",
        "        is_handwritten_dim = 1\n",
        "        self.softplus = nn.Softplus()    #D\n",
        "        self.fc1 = nn.Linear(\n",
        "             img_dim + digit_dim + is_handwritten_dim, hidden_dim\n",
        "        )    #E\n",
        "        self.fc21 = nn.Linear(hidden_dim, z_dim)    #F\n",
        "        self.fc22 = nn.Linear(hidden_dim, z_dim)    #F\n",
        "\n",
        "    def forward(self, img, digit, is_handwritten):    #G\n",
        "        input = torch.cat([img, digit, is_handwritten], dim=1)    #H\n",
        "        hidden = self.softplus(self.fc1(input))    #I\n",
        "        z_loc = self.fc21(hidden)    #J\n",
        "        z_scale = torch.exp(self.fc22(hidden))    #J\n",
        "        return z_loc, z_scale\n",
        "#A The encoder is an instance of a Pytorch module.\n",
        "#B The input image is 28X28 = 784 pixels.\n",
        "#C The digit dimension is 10.\n",
        "#D In the encoder, we’ll only use the softplus transform (activation function).\n",
        "#E The linear transform fc1 combines with the softplus to map the 784 dimensional pixel vector, 10 dimensional digit label vector, and 2 dimensional is_handwritten vector to the hidden layer.\n",
        "#F The linear transforms fc21 and fc22 will combine with the softplus to map the hidden vector to Z’s vector space.\n",
        "#G Define the reverse computation from an observed X variable value to a latent Z variable value.\n",
        "#H Combine the image vector, digit label, and is-handwritten label into one input.\n",
        "#I Map the input to the hidden layer.\n",
        "#J The VAE framework will sample Z from a Normal distribution that approximates P(Z|img, digit, is_handwritten). The final transforms map the hidden layer to a location and scale parameter for that Normal distribution."
      ],
      "metadata": {
        "id": "UGGxsLWIWkvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 5.7: The guide function\n",
        "\n",
        "`training_guide` contains the encoder. The purpose of `training_guide` is to approximate P(Z|image, digit, is_handwritten) during training."
      ],
      "metadata": {
        "id": "TP920mmcqtsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_guide(self, img, digit, is_handwritten, batch_size):    #A\n",
        "    pyro.module(\"encoder\", self.encoder)    #B\n",
        "    options = dict(dtype=torch.float32, device=DEVICE_TYPE)\n",
        "    with pyro.plate(\"data\", batch_size):    #C\n",
        "        z_loc, z_scale = self.encoder(img, digit, is_handwritten)    #D\n",
        "        z = pyro.sample(\"Z\", dist.Normal(z_loc, z_scale).to_event(1))    #E\n",
        "#A training_guide is a method of the VAE which will use the encoder.\n",
        "#B Register the encoder so Pyro is aware of its weight parameters.\n",
        "#C This is the same plate context manager for iterating over the batch data that we see in the training_model.\n",
        "#D Use the encoder to map an image and its labels to parameters of a Normal distribution.\n",
        "#E Sample Z from that Normal distribution."
      ],
      "metadata": {
        "id": "xy2T3-CtZp5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 5.8: The full VAE code\n",
        "\n",
        "Now we implement all the parts in the VAE."
      ],
      "metadata": {
        "id": "Bbit-PBlrayq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        z_dim=50,    #A\n",
        "        hidden_dim=400,    #B\n",
        "        use_cuda=USE_CUDA,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.use_cuda = use_cuda\n",
        "        self.z_dim = z_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.setup_networks()\n",
        "\n",
        "    def setup_networks(self):    #C\n",
        "        self.encoder = Encoder(self.z_dim, self.hidden_dim)\n",
        "        self.decoder = Decoder(self.z_dim, self.hidden_dim)\n",
        "        if self.use_cuda:\n",
        "            self.cuda()\n",
        "\n",
        "    def model(self, data_size=1):    #D\n",
        "        pyro.module(\"decoder\", self.decoder)\n",
        "        options = dict(dtype=torch.float32, device=DEVICE_TYPE)\n",
        "        z_loc = torch.zeros(data_size, self.z_dim, **options)\n",
        "        z_scale = torch.ones(data_size, self.z_dim, **options)\n",
        "        z = pyro.sample(\"Z\", dist.Normal(z_loc, z_scale).to_event(1))\n",
        "        p_digit = torch.ones(data_size, 10, **options)/10\n",
        "        digit = pyro.sample(\n",
        "            \"digit\",\n",
        "            dist.OneHotCategorical(p_digit)\n",
        "        )\n",
        "        p_is_handwritten = torch.ones(data_size, 1, **options)/2\n",
        "        is_handwritten = pyro.sample(\n",
        "            \"is_handwritten\",\n",
        "            dist.Bernoulli(p_is_handwritten).to_event(1)\n",
        "        )\n",
        "        img_param = self.decoder(z, digit, is_handwritten)\n",
        "        img = pyro.sample(\"img\", dist.Bernoulli(img_param).to_event(1))\n",
        "        return img, digit, is_handwritten\n",
        "\n",
        "    def training_model(self, img, digit, is_handwritten, batch_size):   #E\n",
        "        conditioned_on_data = pyro.condition(\n",
        "            self.model,\n",
        "            data={\n",
        "                \"digit\": digit,\n",
        "                \"is_handwritten\": is_handwritten,\n",
        "                \"img\": img\n",
        "            }\n",
        "        )\n",
        "        with pyro.plate(\"data\", batch_size):\n",
        "            img, digit, is_handwritten = conditioned_on_data(batch_size)\n",
        "        return img, digit, is_handwritten\n",
        "\n",
        "    def training_guide(self, img, digit, is_handwritten, batch_size):   #F\n",
        "        pyro.module(\"encoder\", self.encoder)\n",
        "        options = dict(dtype=torch.float32, device=DEVICE_TYPE)\n",
        "        with pyro.plate(\"data\", batch_size):\n",
        "            z_loc, z_scale = self.encoder(img, digit, is_handwritten)\n",
        "            z = pyro.sample(\"Z\", dist.Normal(z_loc, z_scale).to_event(1))\n",
        "#A Setting the latent dimension to have a dimension of 50.\n",
        "#B Setting the hidden layers to have a dimension of 400.\n",
        "#C Setup the encoder and decoder.\n",
        "#D The causal model of an image.\n",
        "#E The causal model of the N images in the training data.\n",
        "#F The guide that encodes the approximating distribution used in variation inference."
      ],
      "metadata": {
        "id": "o5vOr1GXe_3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 5.9 Helper function for plotting images\n",
        "\n",
        "The following utility functions helps us visualize progress during training."
      ],
      "metadata": {
        "id": "e8_IhBKK0kpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image(img, title=None):    #A\n",
        "    fig = plt.figure()\n",
        "    plt.imshow(img.cpu(), cmap='Greys_r', interpolation='nearest')\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "#A Helper function for plotting an image"
      ],
      "metadata": {
        "id": "ohxiEjB30lHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 5.10: Define a helper functions for reconstructing and viewing the images\n",
        "\n",
        "These additional utility functions help us selected and reshape images, as well as generate new images."
      ],
      "metadata": {
        "id": "BTFLSyVW06tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def reconstruct_img(vae, img, digit, is_handwritten, use_cuda=USE_CUDA): #A\n",
        "    img = img.reshape(-1, 28 * 28)\n",
        "    digit = F.one_hot(torch.tensor(digit), 10)\n",
        "    is_handwritten = torch.tensor(is_handwritten_rng).unsqueeze(0)\n",
        "    if use_cuda:\n",
        "      img, digit, is_handwritten = img.cuda(), digit.cuda(), is_handwritten.cuda()\n",
        "    z_loc, z_scale = vae.encoder(img, digit, is_handwritten)\n",
        "    z = dist.Normal(z_loc, z_scale).sample()\n",
        "    img_expectation = vae.decoder(z, digit, is_handwritten)\n",
        "    return img_expectation.squeeze().view(28, 28).detach()\n",
        "\n",
        "def compare_images(img1, img2):    #B\n",
        "    fig = plt.figure()\n",
        "    ax0 = fig.add_subplot(121)\n",
        "    plt.imshow(img1.cpu(), cmap='Greys_r', interpolation='nearest')\n",
        "    plt.axis('off')\n",
        "    plt.title('original')\n",
        "    ax1 = fig.add_subplot(122)\n",
        "    plt.imshow(img2.cpu(), cmap='Greys_r', interpolation='nearest')\n",
        "    plt.axis('off')\n",
        "    plt.title('reconstruction')\n",
        "    plt.show()\n",
        "#A Given an input image, \"reconstructs\" the image by passing through the encoder then through the decoder.\n",
        "#B Plots two images side by side for comparison."
      ],
      "metadata": {
        "id": "QqBkhFAy066G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 5.11: Data processing helper functions for training\n",
        "\n",
        "Next, we'll create some helper functions for handling the data. We'll use `get_random_example` to grab random images from the dataset. `reshape_data` will convert an image and its labels into input for the encoder. We'll use `generate_data` and `generate_coded_data` will simulate an image from the model."
      ],
      "metadata": {
        "id": "79MHVV7313mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def get_random_example(loader):    #A\n",
        "    random_idx = np.random.randint(0, len(loader.dataset))    #A\n",
        "    img, digit, is_handwritten = loader.dataset[random_idx]    #A\n",
        "    return img.squeeze(), digit, is_handwritten    #A\n",
        "\n",
        "def reshape_data(img, digit, is_handwritten):    #B\n",
        "    digit = F.one_hot(digit, 10).squeeze()    #B\n",
        "    img = img.reshape(-1, 28*28)    #B\n",
        "    return img, digit, is_handwritten    #B\n",
        "\n",
        "def generate_coded_data(vae, use_cuda=USE_CUDA):    #C\n",
        "    z_loc = torch.zeros(1, vae.z_dim)    #C\n",
        "    z_scale = torch.ones(1, vae.z_dim)    #C\n",
        "    z = dist.Normal(z_loc, z_scale).to_event(1).sample()    #C\n",
        "    p_digit = torch.ones(1, 10)/10    #C\n",
        "    digit = dist.OneHotCategorical(p_digit).sample()    #C\n",
        "    p_is_handwritten = torch.ones(1, 1)/2    #C\n",
        "    is_handwritten = dist.Bernoulli(p_is_handwritten).sample()    #C\n",
        "    if use_cuda:    #C\n",
        "        z, digit, is_handwritten = z.cuda(), digit.cuda(), is_handwritten.cuda()    #C\n",
        "    img = vae.decoder(z, digit, is_handwritten)    #C\n",
        "    return img, digit, is_handwritten    #C\n",
        "\n",
        "def generate_data(vae, use_cuda=USE_CUDA):    #D\n",
        "    img, digit, is_handwritten = generate_coded_data(vae, use_cuda)    #D\n",
        "    img = img.squeeze().view(28, 28).detach()    #D\n",
        "    digit = torch.argmax(digit, 1)    #D\n",
        "    is_handwritten = torch.argmax(is_handwritten, 1)    #D\n",
        "    return img, digit, is_handwritten    #D\n",
        "#A Chose a random example from the dataset.\n",
        "#B Reshape the data.\n",
        "#C Generate data that is encoded.\n",
        "#D Generate (unencoded) data."
      ],
      "metadata": {
        "id": "fmSgUQrl1tFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 5.12: Set up the training procedure\n",
        "\n",
        "Next we set up traing. The training objective `Trace_ELBO` simultaneously trains the parameters of the encoder and the decoder. It focuses on minimizing reconstruction error (how much information is lost when an image encoded, and then decoded once again) and [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the distribution modeled by the guide (the variational distribution) and the P(Z|image, is_handwritten, digit)."
      ],
      "metadata": {
        "id": "wFp3sIBu2-eW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 256\n",
        "LEARNING_RATE = 1.0e-3\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "\n",
        "pyro.clear_param_store()    #A\n",
        "vae = VAE()    #B\n",
        "\n",
        "train_loader, test_loader = setup_dataloaders(batch_size=BATCH_SIZE, use_cuda=USE_CUDA)    #C\n",
        "train_size = len(train_loader.dataset)    #C\n",
        "test_size = len(test_loader.dataset)    #C\n",
        "\n",
        "svi_adam = Adam({\"lr\": LEARNING_RATE})    #D\n",
        "svi = SVI(vae.training_model, vae.training_guide, svi_adam, loss=Trace_ELBO())    #E\n",
        "#A Clear any values of the parameters in the guide memory.\n",
        "#B Initalize the VAE\n",
        "#C Load the data\n",
        "#D Initialize the optizer\n",
        "#E Initialize the SVI loss calculator. Loss negative \"expected lower bound\" (ELBO)."
      ],
      "metadata": {
        "id": "iEbyt3uo3Fhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Listing 5.13: Running training and plotting progress\n",
        "\n",
        "Finally, we run training."
      ],
      "metadata": {
        "id": "pmvAUfzZ3X4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 2500\n",
        "LEARNING_RATE = 1.0e-3\n",
        "TEST_FREQUENCY = 10\n",
        "\n",
        "train_loss, test_loss = [], []\n",
        "\n",
        "for epoch in range(0, NUM_EPOCHS+1):    #A\n",
        "    epoch_loss_train = 0\n",
        "    for batch_idx, (img, digit, is_handwritten) in enumerate(train_loader):\n",
        "        batch_size = img.shape[0]\n",
        "        if USE_CUDA:\n",
        "            img = img.cuda()\n",
        "            digit = digit.cuda()\n",
        "            is_handwritten = is_handwritten.cuda()\n",
        "        img, digit, is_handwritten = reshape_data(\n",
        "            img, digit, is_handwritten\n",
        "        )\n",
        "        epoch_loss_train += svi.step(    #B\n",
        "            img, digit, is_handwritten, batch_size    #B\n",
        "        )    #B\n",
        "    epoch_loss_train = epoch_loss_train / train_size\n",
        "    print(\"Epoch: {} average training loss: {}\".format(epoch, epoch_loss_train))\n",
        "    train_loss.append(epoch_loss_train)\n",
        "    if epoch % TEST_FREQUENCY == 0:    #C\n",
        "        epoch_loss_test = 0\n",
        "        for batch_idx, (img, digit, is_handwritten) in enumerate(test_loader):\n",
        "            batch_size = img.shape[0]\n",
        "            if USE_CUDA:\n",
        "                img = img.cuda()\n",
        "                digit = digit.cuda()\n",
        "                is_handwritten = is_handwritten.cuda()\n",
        "            img, digit, is_handwritten = reshape_data(\n",
        "                img, digit, is_handwritten\n",
        "            )\n",
        "            epoch_loss_test += svi.evaluate_loss(\n",
        "                img, digit, is_handwritten, batch_size\n",
        "            )\n",
        "        epoch_loss_test = epoch_loss_test/test_size\n",
        "        print(\"Epoch: {} average test loss: {}\".format(epoch, epoch_loss_test))\n",
        "        print(\"Comparing a random test image to its reconstruction:\")\n",
        "        img_rng, digit_rng, is_handwritten_rng = get_random_example(test_loader)\n",
        "        img_recon = reconstruct_img(vae, img_rng, digit_rng, is_handwritten_rng)\n",
        "        compare_images(img_rng, img_recon)\n",
        "        print(\"Generate a random image from the model:\")\n",
        "        img_gen, digit_gen, is_handwritten_gen = generate_data(vae)\n",
        "        plot_image(img_gen, \"Generated Image\")\n",
        "        print(\"Intended digit: \", int(digit_gen))\n",
        "        print(\"Intended as handwritten: \", bool(is_handwritten_gen == 1))\n",
        "#A Run the training procedure for a certain number of epochs.\n",
        "#B Run a training step on one batch in one epoch.\n",
        "#C At certain points, the procedure prints generated and reconstructed images."
      ],
      "metadata": {
        "id": "krc0vcPv3Zqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3m6t3b8G4Fg"
      },
      "source": [
        "We can continue to use `generate_data` to generate from the model once we've trained it. Finally, we can save the resulting model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxgJvMrecHJ5"
      },
      "outputs": [],
      "source": [
        "#torch.save(vae.state_dict(), 'mnist_tmnist_weights_March11.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
